# ğŸ” LIME vs SHAP: Explainability for Tabular Machine Learning

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![XAI](https://img.shields.io/badge/topic-Explainable%20AI-orange.svg)]()

This project compares two popular post-hoc explainability methods â€” **LIME** and **SHAP** â€” on a Random Forest classifier
trained on the Breast Cancer dataset. It is designed as a mini research project for an advanced AI/ML course.

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ xai_comparison.ipynb
â”œâ”€â”€ slides/
â”‚   â””â”€â”€ xai.slides.pptx.pdf

```

## ğŸš€ Getting Started

### 1. Install Dependencies

\`\`\`bash
pip install shap lime scikit-learn pandas matplotlib seaborn gradio
\`\`\`

### 2. Run the Notebook

\`\`\`bash
jupyter notebook notebooks/xai_comparison.ipynb
\`\`\`



## ğŸ“Š Methods Compared

### âœ” LIME (Local Interpretable Model-Agnostic Explanations)
- Local surrogate model around an individual prediction.
- Pros: fast, model-agnostic, intuitive.
- Cons: sensitive to sampling; not a global view.

### âœ” SHAP (SHapley Additive exPlanations)
- Based on Shapley values from cooperative game theory.
- Pros: strong theoretical grounding, global + local interpretability.
- Cons: more computationally expensive.



ğŸ‘‰ **YouTube link https://www.youtube.com/watch?v=pr3DKUJ-1Rs .**

## ğŸ“š References

- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). *\"Why Should I Trust You?\" Explaining the Predictions of Any Classifier.* KDD.  
- Lundberg, S. M., & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions.* NIPS.  
- SHAP documentation: https://shap.readthedocs.io  
- LIME documentation: https://github.com/marcotcr/lime  
